{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1p5wamGuy0V8BmMWQ7CfKqlo6uXRNGmbO","authorship_tag":"ABX9TyPX3gJy4kSTAWe6/GeuGHqo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"id":"YCjAUn-Ogwtv","executionInfo":{"status":"ok","timestamp":1733355410488,"user_tz":300,"elapsed":102,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}}},"outputs":[],"source":["# load libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn import linear_model\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","import xgboost as xgb\n","from scipy import stats\n","\n","import warnings\n","\n","# suppress all warnings\n","warnings.filterwarnings('ignore')\n","\n","# set seed\n","np.random.seed(42)"]},{"cell_type":"code","source":["# specify file paths\n","train_df = \"train_subset.csv\"\n","train_targets = \"train_targets.csv\"\n","test_df = \"test.csv\"\n","\n","# read in files\n","X_train = pd.read_csv(train_df)\n","y_train = pd.read_csv(train_targets)['AAC']   # keep only AAC column\n","X_test = pd.read_csv(test_df)\n","\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)"],"metadata":{"id":"FJUsIM2FlVxT","executionInfo":{"status":"ok","timestamp":1733355166390,"user_tz":300,"elapsed":3544,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24a84598-9f06-4690-ea58-312efa05ff07"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(742, 457)\n","(742,)\n","(304, 19921)\n"]}]},{"cell_type":"code","source":["# filter to keep only relevant genes\n","X_test = X_test[X_train.columns]\n","X_test.shape\n","\n","# create train val splits to get internal spearman corr\n","Xtr, Xte, ytr, yte = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"],"metadata":{"id":"2-6cINEEnela","executionInfo":{"status":"ok","timestamp":1733355414351,"user_tz":300,"elapsed":105,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Submission 13: Elastic Net\n","\n","Public score: 0.44524"],"metadata":{"id":"8qjlDgTlOX2r"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 1,\n","                              max_iter = 1000)\n","\n","# fit model\n","en.fit(X_train, y_train)\n","\n","# get predicted values for test data\n","y_pred = en.predict(X_test)\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"enpred.csv\")\n","\n","# get correlation\n","en.fit(Xtr, ytr)\n","y_pred = en.predict(Xte)\n","print(\"Spearman correlation:\", stats.spearmanr(y_pred, yte)[0])"],"metadata":{"id":"5i-Vht3XOVfk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733355416408,"user_tz":300,"elapsed":102,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}},"outputId":"de373e48-c492-4a2e-b39e-252e9e3225e2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman correlation: 0.38029056801264327\n"]}]},{"cell_type":"markdown","source":["Submission 12: Elastic Net\n","\n","Public score: 0.41931"],"metadata":{"id":"pRh9j6kxM6nH"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.8,\n","                              max_iter = 1000)\n","\n","# fit model\n","en.fit(X_train, y_train)\n","\n","# get predicted values for test data\n","y_pred = en.predict(X_test)\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"enpred.csv\")\n","\n","# get correlation\n","en.fit(Xtr, ytr)\n","y_pred = en.predict(Xte)\n","print(\"Spearman correlation:\", stats.spearmanr(y_pred, yte)[0])"],"metadata":{"id":"VnTbEV3YM5Rf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733355422528,"user_tz":300,"elapsed":87,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}},"outputId":"f2b37dfb-e182-44db-851c-d477b8ced8a9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman correlation: 0.40229131352393976\n"]}]},{"cell_type":"markdown","source":["Submission 11: Elastic Net\n","\n","Public score: 0.29042"],"metadata":{"id":"11qtnLngLkj_"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.2,\n","                              max_iter = 1000)\n","\n","# fit model\n","en.fit(X_train, y_train)\n","\n","# get predicted values for test data\n","y_pred = en.predict(X_test)\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"enpred.csv\")\n","\n","# get correlation\n","en.fit(Xtr, ytr)\n","y_pred = en.predict(Xte)\n","print(\"Spearman correlation:\", stats.spearmanr(y_pred, yte)[0])"],"metadata":{"id":"1gHJKdTbLiWs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733355324416,"user_tz":300,"elapsed":300,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}},"outputId":"30188892-082a-4b73-da89-4fad7f89a922"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman correlation: 0.36617845653784015\n"]}]},{"cell_type":"markdown","source":["Submission 10: Ensemble 3: Elastic Net and LASSO\n","\n","Public score: 0.35215"],"metadata":{"id":"CIl01CV1EcHr"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                             l1_ratio = 0.5,\n","                             max_iter = 1000)\n","ls = linear_model.Lasso(alpha = 0.01,\n","                        max_iter = 500)\n","\n","# elastic net\n","en.fit(X_train, y_train)\n","en_y_pred = en.predict(X_test)\n","\n","# scale the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# lasso\n","ls.fit(X_train, y_train)\n","ls_y_pred = ls.predict(X_test)\n","\n","# average predictions\n","y_pred = (en_y_pred * 0.6 + ls_y_pred * 0.4) / 2\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"ensemble3pred.csv\")"],"metadata":{"id":"eW82ovAxEZ5W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 9: Elastic Net\n","\n","Public score: 0.35877"],"metadata":{"id":"1WE8HbESDMJ0"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.5,\n","                              max_iter = 1000)\n","\n","# fit model\n","en.fit(X_train, y_train)\n","\n","# get predicted values for test data\n","y_pred = en.predict(X_test)\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"enpred.csv\")\n","\n","# get correlation\n","en.fit(Xtr, ytr)\n","y_pred = en.predict(Xte)\n","print(\"Spearman correlation:\", stats.spearmanr(y_pred, yte)[0])"],"metadata":{"id":"muzqQ9BXDC7T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733355370803,"user_tz":300,"elapsed":80,"user":{"displayName":"Julia Nguyen","userId":"02226039291033821968"}},"outputId":"3608fb06-4d8a-40a6-fce0-2c7487ce2ca5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman correlation: 0.4096593593864911\n"]}]},{"cell_type":"markdown","source":["Submission 8: Ensemble 1: Elastic Net, Random Forest, and XGBoost\n","\n","Public score: 0.34287"],"metadata":{"id":"Xoj_ahYDDBlY"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.5,\n","                              max_iter = 1000)\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","en.fit(X_train, y_train)\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","en_y_pred = en.predict(X_test)\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (en_y_pred * 0.8 + rf_y_pred * 0.1 + xg_y_pred * 0.1) / 3\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"ensemble1pred.csv\")\n","\n","# get feature importances\n","en_feat = en.coef_\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","en_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': en_feat\n","}).sort_values(by='Weight', ascending=False)\n","rf_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","en_feat.to_csv(\"e1_en_features.csv\", index=False)\n","rf_feat.to_csv(\"e1_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e1_xg_features.csv\", index=False)"],"metadata":{"id":"xnNqdIF0C9kd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 7: Ensemble 1: Elastic Net, Random Forest, and XGBoost\n","\n","Public score: 0.29959"],"metadata":{"id":"OsZcLJFXBABk"}},{"cell_type":"code","source":["# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.5,\n","                              max_iter = 1000)\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      #early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","en.fit(X_train, y_train)\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","en_y_pred = en.predict(X_test)\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (en_y_pred * 0.4 + rf_y_pred * 0.2 + xg_y_pred * 0.2) / 3\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"ensemble1pred.csv\")\n","\n","# get feature importances\n","en_feat = en.coef_\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","en_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': en_feat\n","}).sort_values(by='Weight', ascending=False)\n","rf_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","en_feat.to_csv(\"e1_en_features.csv\", index=False)\n","rf_feat.to_csv(\"e1_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e1_xg_features.csv\", index=False)"],"metadata":{"id":"KkOVJ---A7Mh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 6: Ensemble 1: Elastic Net, Random Forest, and XGBoost\n","\n","Public score: 0.23668"],"metadata":{"id":"0WYpOTHu_moO"}},{"cell_type":"code","source":["# get gene names\n","feat = X_train.columns\n","\n","# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.5,\n","                              max_iter = 1000)\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      #early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# scale the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","en.fit(X_train, y_train)\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","en_y_pred = en.predict(X_test)\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (en_y_pred + rf_y_pred + xg_y_pred) / 3\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"ensemble1pred.csv\")\n","\n","# get feature importances\n","en_feat = en.coef_\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_"],"metadata":{"id":"X7bCtMKv_m1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 5: Ensemble 1: Elastic Net, Random Forest, and XGBoost\n","\n","Public score: 0.27724"],"metadata":{"id":"oPDPZddw-m4F"}},{"cell_type":"code","source":["# get gene names\n","feat = X_train.columns\n","\n","# initialize models\n","en = linear_model.ElasticNet(alpha = 1,\n","                              l1_ratio = 0.5,\n","                              max_iter = 1000)\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      #early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","en.fit(X_train, y_train)\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","en_y_pred = en.predict(X_test)\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (en_y_pred + rf_y_pred + xg_y_pred) / 3\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred})\n","predictions.to_csv(\"ensemble1pred.csv\")\n","\n","# get feature importances\n","en_feat = en.coef_\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","en_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': en_feat\n","}).sort_values(by='Weight', ascending=False)\n","rf_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","en_feat.to_csv(\"e1_en_features.csv\", index=False)\n","rf_feat.to_csv(\"e1_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e1_xg_features.csv\", index=False)"],"metadata":{"id":"QxyEqMPJ-i2y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 3: Ensemble 2: Random Forest and XGBoost\n","\n","Public score: 0.24924"],"metadata":{"id":"GfODx6am4pB9"}},{"cell_type":"code","source":["# get gene names\n","feat = X_train.columns\n","\n","# initialize models\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# scale the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (rf_y_pred + xg_y_pred) / 2\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    #'sampleId': sampleId,\n","    'AAC': y_pred\n","  })\n","predictions.to_csv(\"ensemble2pred.csv\")\n","\n","# get feature importances\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","rf_feat = pd.DataFrame({\n","    'Feat': feat,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Feat': feat,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","rf_feat.to_csv(\"e2_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e2_xg_features.csv\", index=False)"],"metadata":{"id":"KoVl1XI14oLL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 2: Ensemble 2: Random Forest and XGBoost\n","\n","Public score: 0.25283"],"metadata":{"id":"pEdINxwf01FA"}},{"cell_type":"code","source":["# initialize models\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# split the training dataframe into train and val for XGboost\n","X_train_xg, X_test_xg, y_train_xg, y_test_xg = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","# fit models\n","rf.fit(X_train, y_train)\n","xg.fit(X_train_xg, y_train_xg, eval_set = [(X_test_xg, y_test_xg)], verbose=0)\n","\n","# get predicted values for test data\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (rf_y_pred + xg_y_pred) / 2\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    'sampleId': sampleId,\n","    'AAC': y_pred\n","  })\n","predictions.to_csv(\"ensemble2pred.csv\")\n","\n","# get feature importances\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","rf_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","rf_feat.to_csv(\"e2_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e2_xg_features.csv\", index=False)"],"metadata":{"id":"bQoXroEs00Kn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Submission 1: Ensemble 2: Random Forest and XGBoost\n","\n","Public score: 0.19703"],"metadata":{"id":"Q0KY-YQ0t0z3"}},{"cell_type":"code","source":["# initialize models\n","rf = RandomForestRegressor(n_estimators = 200,\n","                            max_depth = 20,\n","                            min_samples_split = 5,\n","                            min_samples_leaf = 1,\n","                            max_features = 'sqrt')\n","xg = xgb.XGBRegressor(tree_method=\"hist\",\n","                      #early_stopping_rounds=2,\n","                      eval_metric=\"rmse\", verbosity=0,\n","                      objective='reg:squarederror',\n","                      max_depth=5, subsample=0.8)\n","\n","# fit models\n","rf.fit(X_train, y_train)\n","xg.fit(X_train, y_train)\n","\n","# get predicted values for test data\n","rf_y_pred = rf.predict(X_test)\n","xg_y_pred = xg.predict(X_test)\n","\n","# average predictions\n","y_pred = (rf_y_pred + xg_y_pred) / 2\n","\n","# save predictions\n","predictions = pd.DataFrame({\n","    'sampleId': sampleId,\n","    'AAC': y_pred\n","  })\n","predictions.to_csv(\"ensemble2pred.csv\")\n","\n","# get feature importances\n","rf_feat = rf.feature_importances_\n","xg_feat = xg.feature_importances_\n","\n","# create feature importance dataframes\n","rf_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': rf_feat\n","}).sort_values(by='Weight', ascending=False)\n","xg_feat = pd.DataFrame({\n","    'Peak': X_train.columns,\n","    'Weight': xg_feat\n","}).sort_values(by='Weight', ascending=False)\n","\n","# save feature importance dataframe\n","rf_feat.to_csv(\"e2_rf_features.csv\", index=False)\n","xg_feat.to_csv(\"e2_xg_features.csv\", index=False)"],"metadata":{"id":"Q6TjN3Q8YVJ_"},"execution_count":null,"outputs":[]}]}